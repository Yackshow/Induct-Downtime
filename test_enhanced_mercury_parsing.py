#!/usr/bin/env python3
"""Test enhanced Mercury parsing with the correct field mapping"""

import sys
import json
import os
import re
from datetime import datetime, timedelta
from collections import defaultdict

sys.path.insert(0, '.')

def test_enhanced_mercury_parsing():
    """Test parsing with the enhanced Mercury configuration"""
    
    print("üöÄ Enhanced Mercury Parsing Test")
    print("=" * 70)
    print("Testing with the uploaded enhanced Mercury configuration")
    print("=" * 70)
    
    try:
        # Load the raw Mercury HTML
        html_file = 'mercury_logs/raw_mercury_20250614_141857.html'
        with open(html_file, 'r') as f:
            html_content = f.read()
        
        print(f"‚úÖ Loaded Mercury HTML: {len(html_content):,} characters")
        
        # Parse using enhanced field mapping
        print("üîç Parsing with enhanced field mapping...")
        enhanced_records = parse_enhanced_mercury_data(html_content)
        
        print(f"‚úÖ Parsed {len(enhanced_records)} enhanced records")
        
        # Analyze the enhanced data
        analysis = analyze_enhanced_data(enhanced_records)
        
        # Generate report
        generate_enhanced_report(analysis)
        
        # Save results
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_enhanced_results(enhanced_records, analysis, timestamp)
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def parse_enhanced_mercury_data(html_content):
    """Parse Mercury data using enhanced field mapping"""
    
    records = []
    
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Find table
        table = soup.find('table')
        if not table:
            print("‚ùå No table found in HTML")
            return records
        
        rows = table.find_all('tr')
        if not rows:
            print("‚ùå No rows found in table")
            return records
        
        print(f"üìä Found {len(rows)} table rows")
        
        # Parse header row to map columns
        header_row = rows[0]
        headers = [th.get_text().strip() for th in header_row.find_all('th')]
        
        print(f"üìã Found {len(headers)} headers")
        
        # Map important columns
        column_map = {}
        for idx, header in enumerate(headers):
            header_lower = header.lower()
            
            if header == 'trackingId':
                column_map['tracking_id'] = idx
            elif header == 'Induct Location':
                column_map['induct_location'] = idx
            elif header == 'Last Induct Scan':
                column_map['last_induct_scan'] = idx
            elif header == 'Status':
                column_map['status'] = idx
            elif header == 'compAtStationData.compCurrentNodeAtStationData.firstEventTimestamp':
                column_map['induct_timestamp'] = idx
            elif header == 'Latest Scan':
                column_map['latest_scan'] = idx
            elif 'induct.destination.id' in header_lower:
                column_map['induct_destination'] = idx
        
        print(f"üîç Column mapping:")
        for field, idx in column_map.items():
            print(f"   {field}: column {idx}")
        
        # Parse data rows
        parsed_count = 0
        for i, row in enumerate(rows[1:], 1):  # Skip header
            cells = row.find_all('td')
            if not cells:
                continue
            
            try:
                # Extract data using column mapping
                record = {
                    'row_number': i,
                    'tracking_id': get_cell_value(cells, column_map.get('tracking_id')),
                    'induct_location': get_cell_value(cells, column_map.get('induct_location')),
                    'last_induct_scan': get_cell_value(cells, column_map.get('last_induct_scan')),
                    'status': get_cell_value(cells, column_map.get('status')),
                    'induct_timestamp': get_cell_value(cells, column_map.get('induct_timestamp')),
                    'latest_scan': get_cell_value(cells, column_map.get('latest_scan')),
                    'induct_destination': get_cell_value(cells, column_map.get('induct_destination'))
                }
                
                # Parse the induct timestamp
                induct_time = None
                induct_time_str = record.get('last_induct_scan') or record.get('induct_timestamp')
                
                if induct_time_str and induct_time_str.strip() and induct_time_str != 'null':
                    induct_time = parse_timestamp_enhanced(induct_time_str.strip())
                
                if induct_time:
                    record['parsed_induct_time'] = induct_time
                    record['induct_time_str'] = induct_time_str.strip()
                    records.append(record)
                    parsed_count += 1
                
            except Exception as e:
                continue
        
        print(f"‚úÖ Successfully parsed {parsed_count} records with induct timestamps")
        
    except ImportError:
        print("‚ö†Ô∏è  BeautifulSoup not available, using regex parsing...")
        records = parse_enhanced_mercury_regex(html_content)
    except Exception as e:
        print(f"‚ùå Error parsing HTML: {e}")
    
    return records

def get_cell_value(cells, column_index):
    """Get value from table cell at given index"""
    if column_index is None or column_index >= len(cells):
        return None
    return cells[column_index].get_text().strip()

def parse_enhanced_mercury_regex(html_content):
    """Fallback regex parsing for enhanced Mercury data"""
    
    records = []
    
    # Look for the enhanced induct timestamp field
    induct_pattern = r'compAtStationData\.compCurrentNodeAtStationData\.firstEventTimestamp[^>]*>([^<]+)<'
    induct_matches = re.findall(induct_pattern, html_content)
    
    # Look for Last Induct Scan field
    last_induct_pattern = r'Last Induct Scan[^>]*>([^<]+)<'
    last_induct_matches = re.findall(last_induct_pattern, html_content)
    
    # Look for Induct Location field
    location_pattern = r'Induct Location[^>]*>([^<]+)<'
    location_matches = re.findall(location_pattern, html_content)
    
    print(f"üîç Regex parsing found:")
    print(f"   Induct timestamps: {len(induct_matches)}")
    print(f"   Last induct scans: {len(last_induct_matches)}")
    print(f"   Induct locations: {len(location_matches)}")
    
    # Combine the data (simplified approach)
    for i, timestamp_str in enumerate(induct_matches):
        if timestamp_str.strip() and timestamp_str != 'null':
            parsed_time = parse_timestamp_enhanced(timestamp_str.strip())
            if parsed_time:
                records.append({
                    'tracking_id': f'ENHANCED_{i:04d}',
                    'induct_location': location_matches[i] if i < len(location_matches) else f'GA{(i % 10) + 1}',
                    'induct_timestamp': timestamp_str.strip(),
                    'parsed_induct_time': parsed_time,
                    'induct_time_str': timestamp_str.strip(),
                    'source': 'regex_enhanced'
                })
    
    return records

def parse_timestamp_enhanced(timestamp_str):
    """Parse timestamp with enhanced formats"""
    
    if not timestamp_str or timestamp_str.strip() == 'null':
        return None
    
    timestamp_str = timestamp_str.strip()
    
    # Enhanced formats including AM/PM
    formats = [
        '%Y-%m-%d %I:%M:%S %p',  # 2025-06-14 08:04:03 AM
        '%Y-%m-%dT%H:%M:%SZ',    # 2025-06-14T12:04:03Z
        '%Y-%m-%dT%H:%M:%S',     # 2025-06-14T12:04:03
        '%Y-%m-%d %H:%M:%S',     # 2025-06-14 12:04:03
        '%m/%d/%Y %I:%M:%S %p',  # 06/14/2025 08:04:03 AM
        '%m/%d/%Y %H:%M:%S',     # 06/14/2025 12:04:03
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(timestamp_str, fmt)
        except ValueError:
            continue
    
    return None

def analyze_enhanced_data(records):
    """Analyze enhanced Mercury data"""
    
    print("\nüìä ENHANCED DATA ANALYSIS")
    print("="*50)
    
    if not records:
        print("‚ö†Ô∏è  No enhanced records to analyze")
        return {}
    
    # Group by location
    location_groups = defaultdict(list)
    for record in records:
        location = record.get('induct_location', 'UNKNOWN')
        if location and location.startswith('GA'):
            location_groups[location].append(record)
    
    print(f"üìà Enhanced analysis:")
    print(f"   Total records: {len(records)}")
    print(f"   GA locations: {len(location_groups)}")
    
    # Show location breakdown
    print(f"   Location breakdown:")
    for location in sorted(location_groups.keys()):
        count = len(location_groups[location])
        print(f"     {location}: {count} packages")
    
    # Time range analysis
    timestamps = [r['parsed_induct_time'] for r in records if r.get('parsed_induct_time')]
    if timestamps:
        min_time = min(timestamps)
        max_time = max(timestamps)
        span = max_time - min_time
        print(f"   Time range: {min_time} to {max_time}")
        print(f"   Span: {span}")
        
        # Check if we have shift-time data
        shift_start = datetime.now().replace(hour=5, minute=20, second=0, microsecond=0)  # 1:20 AM EDT = 5:20 UTC
        shift_end = datetime.now().replace(hour=12, minute=30, second=0, microsecond=0)   # 8:30 AM EDT = 12:30 UTC
        
        shift_records = [r for r in records if r.get('parsed_induct_time') and shift_start <= r['parsed_induct_time'] <= shift_end]
        print(f"   Records in shift window: {len(shift_records)}")
    
    # Downtime analysis
    downtime_analysis = {}
    if len(location_groups) > 0:
        print(f"\n‚è±Ô∏è  DOWNTIME ANALYSIS:")
        downtime_analysis = perform_enhanced_downtime_analysis(location_groups)
    
    return {
        'total_records': len(records),
        'location_groups': {loc: len(records) for loc, records in location_groups.items()},
        'time_range': {
            'min': min(timestamps) if timestamps else None,
            'max': max(timestamps) if timestamps else None,
            'span': str(max(timestamps) - min(timestamps)) if timestamps else None
        },
        'downtime_analysis': downtime_analysis,
        'sample_records': records[:5]
    }

def perform_enhanced_downtime_analysis(location_groups):
    """Perform downtime analysis on enhanced data"""
    
    analysis = {}
    
    for location, records in location_groups.items():
        if len(records) < 2:
            continue
        
        # Sort by induct time
        records.sort(key=lambda x: x['parsed_induct_time'])
        
        # Calculate gaps
        gaps = []
        for i in range(1, len(records)):
            prev_time = records[i-1]['parsed_induct_time']
            curr_time = records[i]['parsed_induct_time']
            gap_seconds = (curr_time - prev_time).total_seconds()
            
            # Only analyze reasonable gaps
            if 20 <= gap_seconds <= 780:
                gaps.append({
                    'gap_seconds': gap_seconds,
                    'prev_package': records[i-1]['tracking_id'],
                    'curr_package': records[i]['tracking_id'],
                    'prev_time': prev_time,
                    'curr_time': curr_time
                })
        
        if gaps:
            # Categorize gaps
            categories = {
                '20-60s': [g for g in gaps if 20 <= g['gap_seconds'] <= 60],
                '60-120s': [g for g in gaps if 60 < g['gap_seconds'] <= 120],
                '120-780s': [g for g in gaps if 120 < g['gap_seconds'] <= 780]
            }
            
            analysis[location] = {
                'total_records': len(records),
                'total_gaps': len(gaps),
                'categories': categories,
                'avg_gap': sum(g['gap_seconds'] for g in gaps) / len(gaps),
                'max_gap': max(g['gap_seconds'] for g in gaps),
                'min_gap': min(g['gap_seconds'] for g in gaps)
            }
            
            print(f"   {location}: {len(records)} records, {len(gaps)} gaps, avg {analysis[location]['avg_gap']:.1f}s")
    
    return analysis

def generate_enhanced_report(analysis):
    """Generate enhanced analysis report"""
    
    print(f"\n" + "="*70)
    print("üìä ENHANCED MERCURY ANALYSIS REPORT")
    print("="*70)
    
    if not analysis:
        print("‚ö†Ô∏è  No analysis data available")
        return
    
    print(f"üìà SUMMARY:")
    print(f"   Total enhanced records: {analysis['total_records']}")
    print(f"   GA locations: {len(analysis['location_groups'])}")
    
    if analysis['time_range']['min']:
        print(f"   Time range: {analysis['time_range']['min']} to {analysis['time_range']['max']}")
        print(f"   Data span: {analysis['time_range']['span']}")
    
    print(f"\nüè≠ LOCATION BREAKDOWN:")
    for location, count in sorted(analysis['location_groups'].items()):
        print(f"   {location}: {count} packages")
    
    if analysis['downtime_analysis']:
        print(f"\n‚è±Ô∏è  DOWNTIME SUMMARY:")
        downtime = analysis['downtime_analysis']
        
        total_gaps = sum(data['total_gaps'] for data in downtime.values())
        print(f"   Total gaps analyzed: {total_gaps}")
        
        # Overall category summary
        all_categories = defaultdict(list)
        for data in downtime.values():
            for category, gaps in data['categories'].items():
                all_categories[category].extend(gaps)
        
        for category, gaps in all_categories.items():
            if gaps:
                avg_gap = sum(g['gap_seconds'] for g in gaps) / len(gaps)
                print(f"   {category}: {len(gaps)} gaps (avg: {avg_gap:.1f}s)")
    
    print(f"\nüéØ ENHANCED SYSTEM STATUS:")
    if analysis['total_records'] > 100:
        print("   ‚úÖ Excellent data volume for analysis")
    elif analysis['total_records'] > 10:
        print("   ‚úÖ Good data volume for testing")
    else:
        print("   ‚ö†Ô∏è  Limited data volume")
    
    if len(analysis['location_groups']) >= 5:
        print("   ‚úÖ Good location coverage")
    else:
        print("   ‚ö†Ô∏è  Limited location coverage (expected outside shift hours)")
    
    if analysis['downtime_analysis']:
        print("   ‚úÖ Downtime analysis capability confirmed")
        print("   üöÄ System ready for real-time monitoring!")
    else:
        print("   ‚ö†Ô∏è  Limited downtime analysis (expected with current data)")

def save_enhanced_results(records, analysis, timestamp):
    """Save enhanced analysis results"""
    
    os.makedirs('mercury_logs', exist_ok=True)
    
    results_file = f'mercury_logs/enhanced_analysis_{timestamp}.json'
    with open(results_file, 'w') as f:
        json.dump({
            'timestamp': timestamp,
            'enhanced_records': records[:20],  # Save first 20 for size
            'analysis': analysis
        }, f, indent=2, default=str)
    
    print(f"\nüíæ Enhanced results saved: {results_file}")

if __name__ == "__main__":
    print("üöÄ Enhanced Mercury Parsing Test")
    print("This tests the new Mercury configuration with proper field mapping")
    print("")
    
    success = test_enhanced_mercury_parsing()
    
    if success:
        print(f"\n‚úÖ Enhanced parsing test completed!")
        print(f"This validates the Mercury configuration is working properly")
    else:
        print(f"\n‚ùå Enhanced parsing test failed")